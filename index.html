<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ROGR: Relightable 3D Objects using Generative Relighting</title>

  <link href="https://fonts.googleapis.com/css?family=Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/video_comparison.js"></script>

  <style>
  table {
    width: 100%;
    border-collapse: collapse;
    margin: 20px 0;
    font-family: Arial, sans-serif;
  }

  th, td {
    padding: 12px 15px;
    text-align: left;
    border-bottom: 1px solid #ddd;
  }

  th {
    background-color: #4CAF50;
    color: white;
    font-weight: bold;
  }

  tr:nth-child(even) {
    background-color: #f2f2f2;
  }

  tr:hover {
    background-color: #ddd;
  }

  caption {
    font-size: 1.5em;
    margin-bottom: 10px;
    font-weight: bold;
  }
</style>

</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">

            <h1 class="title is-1 publication-title">ROGR: Relightable 3D Objects </br>using Generative Relighting</h1>
            <div class="is-size-4 publication-authors">
              <span class="author-block">
                <a href="https://neurips.cc/Conferences/2025" target="_blank" style="font-size: 120%;"><b>NeurIPS 2025 (Spotlight)</b></a>
              </span>
            </div> 

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://tangjiapeng.github.io/">Jiapeng Tang</a><sup>*1,3</sup>,</span>
  
              <span class="author-block">
                <a href="https://tangjiapeng.github.io/">Matthew Levine</a><sup>*1</sup>,</span>
  
              <span class="author-block">
                    <a href="https://dorverbin.github.io/" target="_blank">Dor Verbin</a><sup>2</sup>,
              </span>
  
              <span class="author-block">
                <a href="http://stephangarbin.com/" target="_blank">Stephan J. Garbin</a><sup>1</sup>,
              </span>
  
              <span class="author-block">
                <a href="https://niessnerlab.org/members/matthias_niessner/profile.html">Matthias Niessner</a><sup>3</sup>,
              </span>
  
              <span class="author-block">
                <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a><sup>1</sup>,
              </span>
  
              <span class="author-block">
                <a href="https://pratulsrinivasan.github.io/" target="_blank">Pratul P. Srinivasan</a><sup>2</sup>,</span>&nbsp;&nbsp;&nbsp;
  
              <span class="author-block">
                <a href="https://henzler.github.io/" target="_blank">Philipp Henzler</a><sup>1</sup>&nbsp;&nbsp;&nbsp;
              </span>
            </div>
  
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>*</sup>Equal Contribution</span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Google Research</span>
              <span class="author-block"><sup>2</sup>Google Deepmind</span>
              <span class="author-block"><sup>3</sup>Technical University of Munich</span>
            </div>
  

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2406.06527" target="_blank" class="external-link button is-normal is-rounded">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- <span class="link-block">
                  <a href="https://github.com/illuminerf/illuminerf_results" target="_blank" class="external-link button is-normal is-rounded">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (for Quantitative Results)</span>
                  </a>
                </span>
              </div> -->
              <div class="publication-links">
                <span class="link-block">
                  <a href="./index.html" id="active-button" class="external-link button is-normal is-rounded">
                    <span class="icon">
                      <i class="fas fa-home"></i>
                    </span>
                    <span>Main Page</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="./stanford_orb.html" class="external-link button is-normal is-rounded">
                    <span class="icon">
                      <i class="fas fa-flask"></i>
                    </span>
                    <span>Stanford-ORB Gallery</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="./tensoir.html" class="external-link button is-normal is-rounded">
                    <span class="icon">
                      <i class="fas fa-flask"></i>
                    </span>
                    <span>TensoIR Gallery</span>
                  </a>
                </span>
               <span class="link-block">
                  <a href="./wild.html" class="external-link button is-normal is-rounded">
                    <span class="icon">
                      <i class="fas fa-flask"></i>
                    </span>
                    <span>In the Wild Gallery</span>
                  </a>
                </span>

              </div>
              <!-- <div class="publication-links">
                <span class="link-block">
                  <a href="https://github.com/illuminerf/illuminerf_results/releases" target="_blank" class="external-link button is-normal is-rounded">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Full Stanford-ORB Benchmark</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/illuminerf/illuminerf_results/releases" target="_blank" class="external-link button is-normal is-rounded">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Full TensoIR Benchmark</span>
                  </a>
                </span>
              </div> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <hr />

  <!-- <section class="section" id="teaser">
    <div class="container is-max-desktop">
      <div class="columns is-mobile has-text-centered is-vcentered ">
        <div class="column is-full is-vcentered">
          <video class="video" loop playsinline muted autoplay controls src="./static/teaser.mp4"></video>
        </div>
      </div>
    </div>
  </section> -->

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-blueshirt">
            <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/stationary/sewing_fireplace.mp4"
                      type="video/mp4">
            </video>
          </div>

          <!-- <div class="item item-sewing">
            <video poster="" id="sewing" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/stationary/beemer_abandoned_2.mp4"
                      type="video/mp4">
            </video>
          </div>
  
          <div class="item item-steve">
            <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/stationary/beemer_abandoned_3.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item item-steve">
            <video poster="" id="steve2" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/stationary/beemer_abandoned_church_1.mp4"
                      type="video/mp4">
            </video>
          </div> -->
          <div class="item item-chair-tp">
            <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/stationary/lego_city.mp4"
                      type="video/mp4">
            </video>
          </div>

          <div class="item item-fullbody">
            <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/hotdog-bridge-poster-sized.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="item item-shiba">
            <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
              <source src="./static/stanford_orb_renders_compare_illuminerf/pepsi_scene002_pepsi_scene003-poster.mp4"
                      type="video/mp4">
            </video>
          </div>
    

          <div class="item item-shiba">
            <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/stationary/lego_fireplace.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
  
        <h2 class="subtitle has-text-centered">
          Given an image collection under unknown lighting, <span class="dnerf">ROGR</span>
          reconstructs a relightable neural radiance field, 
          that can be rendered under any novel environment map without further optimization, 
          on-the-fly relighting and novel view synthesis. 
        </h2>
        
      </div>
    </div>
  </section>

  <hr />

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <h2 class="title is-3">Abstract</h2>
      <div class="content has-text-justified">
        <p>
          We introduce <span class="dnerf">ROGR</span>, a novel approach that reconstructs a relightable 3D model of an object captured from multiple views, driven by a generative relighting model that simulates the effects of placing the object under novel environment illuminations. 
          Our method samples the appearance of the object under multiple lighting environments, creating a dataset that is used to train a lighting-conditioned Neural Radiance Field (NeRF) that outputs the object's appearance under any input enviromental lighting. 
         The lighting-conditioned NeRF uses a novel dual-branch architecture to encode the general lighting effects and specularities separately.
         </p>
         <p>
           The optimized lighting-conditioned NeRF enables efficient feed-forward relighting under arbitrary environment maps without requiring per-illumination optimization or light transport simulation. 
           We evaluate our approach on the established TensoIR and Stanford-ORB datasets, 
           where it improves upon the state-of-the-art on most metrics,  showcase our approach on real-world object captures.
         </p>
      </div>
    </div>
  </section>

  <hr />




  <section class="section">
    <div class="container is-max-desktop">
      <!-- method. -->
      <h2 class="title is-3">Method</h2>
      <!-- <div class="columns is-mobile has-text-centered is-size-7-mobile is-vcentered "> -->
      <!-- <div class="column is-full">
        <img src="./static/overview.png" />
        <br/>
        <div class="content has-text-left">
          <ol>
            <li>Given a set of images and camera poses in (a), we run NeRF to extract the 3D geometry as in (b);</li>
            <li>Based on this geometry and a target light shown in (c), we create radiance cues for each given input view as in (d);</li>
            <li>Next, we independently relight each input image using a Relighting Diffusion Model illustrated in (e) and sample S possible solutions for each given image displayed in (f);</li>
            <li>Finally, we distill the relit set of images into a 3D representation through a Latent NeRF optimization as in (g) and (h).</li>
          </ol>
        </div>
        <br/>
      </div> -->
      <!-- </div> -->

      <!-- <p>Relighting results </p> <br> -->
      <div class="content has-text-justified">
        <label class="label" style="width: 100%; vertical-jpg: middle;margin: 0; padding: 0;">
          <img src="./static/images/mvrelit.png" border="0" class="zoom">
        </label>
            <p>Our multi-view relighting diffusion model takes as input N posed images illuminated with a consistent, but unknown illumination, represented by the camera raymap and the source pixel values, and an environment map per image, that has been rotated to the camera pose. The diffusion model generates images of the same object from the same poses, but lit by input environment map. In order to generate our multi-illumination dataset, we repeat this relighting process M times with M different environment maps.</p>

            <p>
            We use a combination of two lighting conditioning signals to train the NeRF on our generated multi-illumination dataset. The general lighting encoding <b>f<sup>general</sup></b> is used for encoding the full environment map in a 
            single embedding and is obtained using a transformer encoder applied to the entire sphere of incident radiance. The specular encoding <b>f<sup>specular</sup></b> is composed of the environment map value, as well as prefiltered versions of the environment map, 
            queried at the reflection direction <b>&omega;<sub>r</sub></b>,
             which is the direction of the camera ray reflected about the surface normal vector. Combining these two conditioning signals provides the NeRF with all the information necessary for relighting diffuse materials as well as shiny ones that exhibit strong reflections.
            </p>
      </div>
      

      <h3 class="title is-4">Multi-view Consistent Relighting</h3>
      <div class="column is-full">
        <div class="content has-text-left">
          <ul style="margin-top: -20px;">
            <li><b>In the left</b>: we show diffusion sample results of single-image relighting model from Neural Gaffer;</li>
            <li><b>In the middle</b>:  we show diffusion sample results of our 16-view relit diffusion model;</li>
            <li><b>In the right</b>: we show the reference ground truths.</li>
          </ul>
        </div>
      </div>

      <div class="columns is-mobile has-text-centered is-size-7-mobile is-vcentered ">
        <div class="column is-one-third">
          <video class="video" loop playsinline muted autoplay controls src="./static/diffusion_samples_fps1/lego_bridge_gaffer.mp4"></video>
          <video class="video" loop playsinline muted autoplay controls src="./static/diffusion_samples_fps1/ficus_fireplace_gaffer.mp4"></video>
          <p> Neural Gaffer 1-view diffusion </p>
        </div>
        <div class="column is-one-third">
          <video class="video" loop playsinline muted autoplay controls src="./static/diffusion_samples_fps1/lego_bridge.mp4"></video>
          <video class="video" loop playsinline muted autoplay controls src="./static/diffusion_samples_fps1/ficus_fireplace.mp4"></video>
          <p> Our 16-view diffusion </p>
        </div>
        <div class="column is-one-third">
          <video class="video" loop playsinline muted autoplay controls src="./static/diffusion_samples_fps1/lego_bridge_gt.mp4"></video>
          <video class="video" loop playsinline muted autoplay controls src="./static/diffusion_samples_fps1/ficus_fireplace_gt.mp4"></video>
          <p> Ground truth </p>
        </div>
      </div>


      <h3 class="title is-4">Generalizable and Relightable Neural Radiance Fields </h3>
      <div class="column is-full">
        <div class="content has-text-left">
          <ul style="margin-top: -20px;">
            <li><b>In the left</b>: we show generalized NeRF relighting results based on specular conditioning;</li>
            <li><b>In the middle</b>: we show generalized NeRF relighting results based on global conditioning;</li>
            <li><b>In the right</b>: we show generalized NeRF relighting results based on both global and specular conditioning.</li>
          </ul>
        </div>
      </div>

      <div class="columns is-mobile has-text-centered is-size-7-mobile is-vcentered ">
        <div class="column is-one-third">
          <video class="video" loop playsinline muted autoplay controls src="./static/ablation_study_fps15/hotdog_no_global_100.mp4"></video>
          <video class="video" loop playsinline muted autoplay controls src="./static/ablation_study_fps15/lego_no_global.mp4"></video>
          <p> only Specular Conditioning </p>
        </div>
        <div class="column is-one-third">
          <video class="video" loop playsinline muted autoplay controls src="./static/ablation_study_fps15/hotdog_no_specular_100.mp4"></video>
          <video class="video" loop playsinline muted autoplay controls src="./static/ablation_study_fps15/lego_no_direcct.mp4"></video>
          <p> only General Conditioning </p>
        </div>
        <div class="column is-one-third">
          <video class="video" loop playsinline muted autoplay controls src="./static/ablation_study_fps15/hotdog_both_100.mp4"></video>
          <video class="video" loop playsinline muted autoplay controls src="./static/ablation_study_fps15/lego_both.mp4"></video>
          <p> General and Specular Conditioning </p>
        </div>
        <!-- <div class="column is-one-fourth">
          <video class="video" loop playsinline muted autoplay controls src="./static/ablation_study/both.mp4"></video>
          <p> Ground Truth </p>
        </div> -->
      </div>

    </div>
  </section>

  <!-- <section>
    <div class="container is-max-desktop">
          <h3 class="title is-4">Computational Efficiency </h3>
        <div class="content has-text-justified">
          A main  feature of our method is that once a radiance field has been optimized for a particular scene, re-rendering under novel illumination - <b> even illumination unseen when initially optimized </b> is very efficient. This is highlighted in the following table. IllumiNeRF reports a total of 2.5 hours to optimize their model across all stages. Our model is similarly expensive to fit, (0.5 hours for diffusion sampling and 2 hours for NeRF optimization, although both reported on more expensive hardware). Both methods require only a single, efficient query of the NeRF per rendered image. However, if IllumiNeRF needs to be rendered under a <i>different</i> illumination condition than it was optimized for, it requires fitting an entirely new model; our approach does not.  
          </p>
      </div>
    </div>

      <table>
        <caption>Speed Comparison of Different Methods on TensoIR</caption>
        <thead>
          <tr>
            <th>Method</th>
            <th>Optimization Time</th>
            <th>Render Time (fixed illumination)</th>
            <th>Render Time (novel illumination)</th>
            <th>Hardware</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>IllumiNeRF</td>
            <td>2.5hrs</td>
            <td><1min</td>
            <td>2.5hrs</td>
            <td>A100 + TPUv5</td>
          </tr>
          <tr>
            <td>Ours</td>
            <td>2.5hrs</td>
            <td><1min</td>
            <td><1min</td>
            <td>H100 + TPUv5</td>
          </tr>
        </tbody>
      </table>
  </section> -->

  <section class="section">
    <div class="container is-max-desktop">
      <!-- method. -->
      <h2 class="title is-3">Related Works</h2>
      <div class="column is-full">
        <div class="content has-text-justified">
          <p>
            Check out the following works which also introduce a relighting diffusion model.
          </p>
          <p>
            <a href="https://dilightnet.github.io/">DiLightNet: Fine-grained Lighting Control for Diffusion-based Image Generation</a>  (single image relighting with radiance cues)
          </p>
          <p>
            <a href="https://neural-gaffer.github.io/">Neural Gaffer: Relighting Any Object via Diffusion </a>  (single image relighting, needs to re-optimized for novel lighting. )
          </p>
          <p>
            <a href="https://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/">A Diffusion Approach to Radiance Field Relighting using Multi-Illumination Synthesis</a>   (single image relighting, needs to re-optimized for novel lighting)
          </p>
          <p>
            <a href="https://illuminerf.github.io/">IllumiNeRF: 3D Relighting Without Inverse Rendering</a>  (single image relighting with radiance cues, needs to re-optimized for novel lighting.)
          </p>
          <p>
            <a href="https://arxiv.org/abs/2410.06231">RelitLRM: Generative Relightable Radiance for Large Reconstruction Models</a> (directly generate relightable NeRF from sparse views and the target illumination, but does not guarantee consistent geometry across environment maps.)
          </p>
          <p>
            <a href="https://arxiv.org/abs/2501.18590">DiffusionRenderer: Neural Inverse and Forward Rendering with Video Diffusion Models</a>  (video diffusion model for relighting but lacks 3D consistency and slow inference speed)
          
        </div>
      </div>
  </section>


   <section class="section">
    <div class="container is-max-desktop">
      <!-- method. -->
      <h2 class="title is-3">Acknowledgements</h2>
      <div class="column is-full">
        <div class="content has-text-justified">
          <p>
            We would like to thank Xiaoming Zhao, Rundi Wu, Songyou Peng, Ruiqi Gao, Ben Poole, Aleksander Holynski, Jason Zhang, Jonathan T. Barron, Stan Szymanowicez, Hadi Alzayer, Alex Trevithick, and Jiahui Lei 
            for their valuable contributions. We also extend our gratitude to Shlomi Fruchter, Kevin Murphy, Mohammad Babaeizadeh, Han Zhang and Amir Hertz for training the base text-to-image latent diffusion model.
          </p>
        </div>
      </div>
  </section>
  
  <hr />


<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{tang2025rogr,
        author    = {Jiapeng Tang and Matthew Levine and Dor Verbin and Stephan J. Garbin and Matthias Niessner and Ricardo Martin-Brualla and Pratul P. Srinivasan and Philipp Henzler},
        title     = {{ROGR: Relightable 3D Objects using Generative Relighting}},
        booktitle = {Advances in Neural Information Processing Systems},
        year      = {2025},
    }</code></pre>
    </div>
</section>



  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
